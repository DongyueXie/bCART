% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BADS.R
\name{BADS}
\alias{BADS}
\title{Fit a bayesian additive decision stump model for regression}
\usage{
BADS(X, y, x.test, sigdf = 3, sigquant = 0.9, k = 2, lambda = NA,
  sigest = NA, sigmaf = NA, ntree = 50, ndpost = 200,
  nskip = 100, Tmin = 2, printevery = 100, save_trees = F,
  rule = "bart", pre_train = T, n_pre_train = 100)
}
\arguments{
\item{X}{samples by features matrix}

\item{y}{response}

\item{x.test}{test samples by feature matrix}

\item{sigdf, sigquant, k, lambda, sigest, sigmaf}{see ?BART::wbart}

\item{ntree}{number of decison stumps}

\item{nskip, ndpost}{number of burn-in and posterior draws}

\item{Tmin}{minimum number of samples in a leaf node allowed}

\item{printevery}{print progress for every 'printevery' iterations}

\item{save_trees}{whether save all the trees from each iteration as a list}

\item{rule}{The splitting rule of a node. Choices are: 1. "grp": Gaussian random projection, randomly draw a length p vector from standard normal as the linear combination coefficients of p variables; 2. sgrp: sparse Gaussian random projection, which generates sparse linear combination coefficients; 3. bart: originla bart splits, which are axis-aligned splits; 4. hyperplane: randomly connect two points from the node as the partiton of node space.}

\item{pre_train}{whether pre-train the model using 'bart' rule before switching to another splitting rule.}

\item{n_pre_train}{number of iterations of pre-train}
}
\value{
BADS returns a list of the following elements.
\item{yhat.train}{A matrix with ndpost rows and nrow(X) columns.}
\item{yhat.test}{A matrix with ndpost rows and nrow(x.test) columns.}
\item{yhat.train.mean}{Posterior mean of MCMC draws of traning data fits}
\item{yhat.test.mean}{Posterior mean of MCMC draws of testing data fits}
\item{sigma}{draws of random error vairaince, length = nskip+ndpost}
\item{tree_history}{If save_trees = TRUE, then a list of all trees}
}
\description{
Bayesian additive decision stump(BADS) is a Bayesian sum of two-leaf-node trees model.
}
\references{
Chipman, H., George, E., and McCulloch R. (2010) Bayesian Additive Regression Trees. The Annals of Applied Statistics, 4,1, 266-298 <doi:10.1214/09-AOAS285>.
}
\author{
Dongyue Xie: \email{dongyxie@gmail.com}
}
